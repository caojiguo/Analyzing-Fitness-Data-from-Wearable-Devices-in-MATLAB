{"name":"Analyzing Fitness Data from Wearable Devices in MATLAB","tagline":"Predictive Modeling of Weight Lifting Dataset with Random Forest","body":"Analyzing Fitness Data from Wearable Devices in MATLAB\r\n======================================================\r\n\r\nCollecting and tracking health and fitness data with smartphones and wearable devices is about to become a mainstream as the internet giants like Apple, Google and Samsung jumping into the fray. But if you collect data, what's the point if you don't analyze it!\r\n\r\nI would like to use a publicly available dataset about weight lifting and pretend that we are going to build a mobile app to advise end users whether they are performing the exercise correctly, and if not, which common mistakes they are making. We will use machine learning/predictive modeling technique called 'Random Forest' popularized by Kaggle competitions to detect user activity patterns.\r\n\r\nDataset to analyze\r\n------------------\r\n\r\n[Human Activity Recognition (HAR)](http://groupware.les.inf.puc-rio.br/har) Weight Lifting Exercise Dataset provides measurements to determine \"how well an activity was performed\". Six subjects performed one set of 10 Unilateral Dumbbell Biceps Curl in 5 different ways.\r\n\r\n1. exactly according to the specification (Class A)\r\n2. throwing the elbows to the front (Class B)\r\n3. lifting the dumbbell only halfway (Class C)\r\n4. lowering the dumbbell only halfway (Class D)\r\n5. throwing the hips to the front (Class E)\r\n\r\nAll together 39,242 samples with 159 variables are available in the original dataset, but we will use a reduced subset of 19,622 samples.\r\n\r\nSensors were placed on the subjects' belts, armbands, glove and dumbbells, as described below:\r\n\r\n![Sensor placement schema](html/on-body-sensing-schema.png)\r\n\r\n<cite>Citation Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. Read more: [http://groupware.les.inf.puc-rio.br/har#ixzz34dpS6oks](http://groupware.les.inf.puc-rio.br/har#ixzz34dpS6ok)</cite>\r\n\r\n```\r\n% load the dataset\r\n% T = readtable('WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv','TreatAsEmpty','NA');\r\nT = readtable('subset.csv','TreatAsEmpty','NA'); % use a subset instead\r\n\r\n% separate the classification label from the predictor variables\r\nclass = categorical(T.classe); T.classe = [];\r\nfprintf('Num samples in data   : %d\\n',height(T))\r\nfprintf('Num predictors in data: %d\\n\\n',width(T))\r\n```\r\n\r\nPartition the dataset for cross validation\r\n------------------------------------------\r\n\r\nOne of the key techniques you use in predictive modeling or machine learning is [cross validation](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29). Roughly speaking, you hold out part of available data for testing later, and build models using the remaining dataset. The held out set is called 'test set' and the set we will use for modeling is called 'training set'. This makes it more difficult to [overfit](http://en.wikipedia.org/wiki/Overfitting) your model, because you can test your model against the data you didn't use in the modeling process.\r\n\r\n```\r\n% Set the random number seed to make the results repeatable\r\nrng('default');\r\n\r\n% Partition the dataset to a 80:20 split\r\ncv = cvpartition(height(T),'holdout',0.20);\r\n\r\n% Training set\r\nXtrain = T(training(cv),:);\r\nYtrain = class(training(cv));\r\nfprintf('\\nTraining Set\\n')\r\ntabulate(Ytrain)\r\n\r\n% Test set\r\nXtest = T(test(cv),:);\r\nYtest = class(test(cv));\r\nfprintf('\\nTest Set\\n')\r\ntabulate(Ytest)\r\n```\r\n```\r\nTraining Set\r\n  Value    Count   Percent\r\n      A     4446     28.32%\r\n      B     3064     19.52%\r\n      C     2771     17.65%\r\n      D     2557     16.29%\r\n      E     2860     18.22%\r\n\r\nTest Set\r\n  Value    Count   Percent\r\n      A     1134     28.90%\r\n      B      733     18.68%\r\n      C      651     16.59%\r\n      D      659     16.79%\r\n      E      747     19.04%\r\n```\r\n\r\nClean and normalize the dataset\r\n-------------------------------\r\n\r\nRaw data is never clean, so you need to check for missing values and transform data into usable form. If the range of values differ substantially from one variable to another, it can affect the machine learning algorithms. Therefore we normalize the data as well.\r\n\r\n```\r\n%  check for missing values\r\nmissingVals = sum(ismissing(Xtrain));\r\nfprintf('\\nNum vars with missind vals: %d\\n',sum(missingVals > 0))\r\nfprintf('Max num missing vals      : %d/%d\\n',max(missingVals),height(Xtrain))\r\nfprintf('Avg num missing vals      : %d/%d\\n\\n',mean(missingVals(missingVals > 0)),height(Xtrain))\r\n\r\n% too many values missing for those variables to be useful\r\n% dropping vars with missing values \r\nXtrain = Xtrain(:,missingVals == 0); Xtest = Xtest(:,missingVals == 0);\r\n\r\n% separate users\r\nusers = double(categorical(Xtrain.user_name));\r\n\r\n% remove user name, time stamps, etc.\r\nXtrain(:,1:6) =[]; Xtest(:,1:6) =[];\r\n\r\n% apply normalization to each variable in training set\r\narr = table2array(Xtrain); mu = mean(arr); sigma = std(arr);\r\nshiftMean2zero = bsxfun(@minus,arr,mu); \r\nscalebySigma = bsxfun(@rdivide,shiftMean2zero,sigma);\r\nXtrain = array2table(scalebySigma,'VariableNames',Xtrain.Properties.VariableNames);\r\n\r\n% apply normalzation to each variable in test set \r\n% with mu and sigma from training set (because test set mu and sigma will\r\n% not be available in actual predictive use of the model).\r\narr = table2array(Xtest);\r\nshiftMean2zero = bsxfun(@minus,arr,mu); \r\nscalebySigma = bsxfun(@rdivide,shiftMean2zero,sigma);\r\nXtest = array2table(scalebySigma,'VariableNames',Xtest.Properties.VariableNames);\r\ndisp('Vars with missing vals and others removed.')\r\n```\r\n```\r\nNum vars with missind vals: 100\r\nMax num missing vals      : 15365/15698\r\nAvg num missing vals      : 15365/15698\r\n\r\nVars with missing vals and others removed.\r\n```\r\n\r\nExploratory Data Analysis\r\n-------------------------\r\n\r\nYou begin exploratory data analysis by plotting the variables in order to get oriented with the dataset. Plots of the first four variables show that:\r\n\r\n1. data is sorted by class - requires random reshuffling.\r\n2. data points cluster around a few different mean values - indicating that measurements were taken by devices calibrated in a few different ways. The following plot shows the first variable by users, and it is clear that one group used one calibration and another group used a different one.\r\n3. those variables exhibit a distinct patterns for Class E (colored in magenta)- those variables will be useful to isolate it.\r\n\r\n```\r\nfigure\r\nsubplot(2,2,1)\r\ngscatter(1:height(Xtrain),Xtrain.roll_belt,Ytrain,'bgrcm','o',5,'off')\r\nxlim([1 height(Xtrain)]);title('Colored by 5 activity type')\r\nsubplot(2,2,2)\r\ngscatter(1:height(Xtrain),Xtrain.pitch_belt,Ytrain,'bgrcm','o',5,'off')\r\nxlim([1 height(Xtrain)]);title('Colored by 5 activity type')\r\nsubplot(2,2,3)\r\ngscatter(1:height(Xtrain),Xtrain.yaw_belt,Ytrain,'bgrcm','o',5,'off')\r\nxlim([1 height(Xtrain)]);title('Colored by 5 activity type')\r\nsubplot(2,2,4)\r\ngscatter(1:height(Xtrain),Xtrain.total_accel_belt,Ytrain,'bgrcm','o',5,'off')\r\nxlim([1 height(Xtrain)]);title('Colored by 5 activity types')\r\n\r\n% plot the first variable by user\r\nfigure\r\ngscatter(1:height(Xtrain),Xtrain.roll_belt,users,'bgrcmy','o',5,'off')\r\nylabel('roll belt'); xlabel('index'); title('Colored by 6 users')\r\n```\r\n![4 features colored by activity type](html/HAR_01.png)\r\n![1st feature colored by user](html/HAR_02.png)\r\n\r\nPredictive Modeling with Random Forest\r\n--------------------------------------\r\n\r\nThe dataset has a bit of issues with calibration. We could further preprocess the data in order to remove calibration gaps. This time, however, I would like to use the dataset as is and use a highly flexible algorithm called [Random Forest](http://en.wikipedia.org/wiki/Random_forest). In MATLAB, this algorithm is implemented in [TreeBagger](http://www.mathworks.com/help/stats/treebagger.html) class available in [Statistics Toolbox](http://www.mathworks.com/products/statistics/).\r\n\r\nRandom Forest became popular particularly after it was used by number of winners in [Kaggle competitions](http://www.kaggle.com/). It uses a large ensemble of decision trees (thus 'forest') trained on different subset of data and uses majority votes of those trees to predict the result. It tend to produce a highly accurate result, but the complexity of the algorithm makes it slow and difficult to interpret.\r\n\r\nTo accelerate the computation, I will enable parallel option supported on [Parallel Computing Toolbox](http://www.mathworks.com/products/parallel-computing/). You can comment out unnecessary code if you don't use it.\r\n\r\nOnce the model is built, you will see the [confusion matrix](http://www.mathworks.com/help/stats/confusionmat.html) that compares the actual class labels to predicted class labels. If everything line up on a diagonal line, then you got 100% accuracy. Off-diagonal numbers are misclassification errors.\r\n\r\nThe model has a very high prediction accuracy even though we saw earlier that our dataset was not as problem free as we would like.\r\n\r\n```\r\n% initialize parallel option - comment out if you don't use parallel\r\npoolobj = gcp('nocreate'); % don't create a new pool even if no pool exits\r\nif isempty(poolobj)\r\n    parpool('local',2)\r\nend\r\nopts = statset('UseParallel',true);\r\n\r\n% reshuffle the dataset\r\nrandidx = randperm(height(Xtrain));\r\nXtrain = Xtrain(randidx,:);\r\nYtrain = Ytrain(randidx);\r\n\r\n% create a random forest model with 100 trees, parallel enabled...\r\nrfmodel = TreeBagger(100,table2array(Xtrain),Ytrain,'options',opts,...\r\n    'Method','classification','oobvarimp','on');\r\n% non-parallel version of the same model\r\n% rfmodel = TreeBagger(100,table2array(Xtrain),Ytrain,...\r\n%     'Method','classification','oobpre','on','oobvarimp','on');\r\n\r\n% predict the class labels for test set\r\n[Ypred,Yscore]= predict(rfmodel,table2array(Xtest));\r\n\r\n% compute the confusion matrix and prediction accuracy\r\nC = confusionmat(Ytest,categorical(Ypred));\r\ndisp(array2table(C,'VariableNames',rfmodel.ClassNames,'RowNames',rfmodel.ClassNames))\r\nfprintf('Prediction accuracy on test set: %f\\n\\n', sum(C(logical(eye(5))))/sum(sum(C)))\r\n```\r\n```\r\nStarting parallel pool (parpool) using the 'local' profile ... connected to 2 workers.\r\n\r\nans = \r\n\r\n Pool with properties: \r\n\r\n            Connected: true\r\n           NumWorkers: 2\r\n              Cluster: local\r\n        AttachedFiles: {}\r\n          IdleTimeout: 30 minute(s) (30 minutes remaining)\r\n          SpmdEnabled: true\r\n\r\n          A       B      C      D      E \r\n         ____    ___    ___    ___    ___\r\n\r\n    A    1133      0      0      0      1\r\n    B       3    730      0      0      0\r\n    C       0      6    643      2      0\r\n    D       0      0     10    649      0\r\n    E       0      0      0      5    742\r\n\r\nPrediction accuracy on test set: 0.993119\r\n```\r\nPlot misclassification errors by number of trees\r\n------------------------------------------------\r\n\r\nI happened to pick 100 trees in the model, but you can check the misclassification errors relative to the number of trees used in prediction. The plot shows that 100 is an overkill - we could use fewer trees and that will make it go faster.\r\n\r\n```\r\nfigure\r\nplot(oobError(rfmodel));\r\nxlabel('Number of Grown Trees');\r\nylabel('Out-of-Bag Classification Error');\r\n```\r\n![Misclassification Error Plot](html/HAR_03.png)\r\n\r\nVariable Importance\r\n-------------------\r\n\r\nOne major criticism of Random Forest is that it is a black box algorithms and not easy to understand what it is doing. However, Random Forest can provide variable importance measure, which correspond to the change in prediction error with and without the presence of that variable in the model.\r\n\r\nFor our hypothetical weight lifting trainer mobile app, Random Forest would be too cumbersome and slow to implement, so you want to use a simpler prediction model with fewer predictor variables. Random Forest can help you with selecting which predictors you can drop without sacrificing the prediction accuracy too much.\r\n\r\nLet's see how you can do this with TreeBagger.\r\n\r\n```\r\n% get the variable importance scores and sort it. \r\nvars = Xtrain.Properties.VariableNames;\r\n% because we turned 'oobvarimp' to 'on', the model contains \r\n% OOBPermutedVarDeltaError that acts as variable importance measure\r\nvarimp = rfmodel.OOBPermutedVarDeltaError';\r\n[~,idxvarimp]= sort(varimp);\r\nlabels = vars(idxvarimp);\r\n\r\n% plot the sorted scores\r\nfigure\r\nbarh(varimp(idxvarimp),1); ylim([1 52]);\r\nset(gca, 'YTickLabel',labels, 'YTick',1:numel(labels))\r\ntitle('Variable Importance'); xlabel('score')\r\n```\r\n![Variable Importance Plot](html/HAR_04.png)\r\n\r\nEvaluate trade-off with ROC plot\r\n--------------------------------\r\n\r\nNow let's do the trade-off between the number of predictor variables and prediction accuracy. [Receiver operating characteristic (ROC)](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves provides a convenient way to visualize and compare performance of binary classifiers. You plot false positive rate against true positive rate at various prediction threshold to produce the curves. If you get completely random result, the curve should follow a diagonal line. If you get a 100% accuracy, then the curve should hug the upper left corner. This means you can use the area under curve (AUC) to evaluate how well each model performs.\r\n\r\nLet's plot ROC curves with different set of predictor variables, using \"C\" class as the positive class, since we can only do this one class at a time, and the previous confusion matrix shows more misclassification errors for this class than others. You can use [perfcurve](http://www.mathworks.com/help/stats/perfcurve.html) to compute ROC curves.\r\n\r\n```\r\n% sort variable importance scores in a descending order\r\n[~,idxvarimp]= sort(varimp,'descend');\r\n\r\n% specify the positive class to use\r\nposLabel = 'C';\r\nposOrder = find(strcmp(posLabel,rfmodel.ClassNames));\r\n\r\n% initialize some accumulators\r\ncolors = lines(7);\r\ncurves = zeros(1,7);\r\nlabels = cell(7,1);\r\n\r\n% plot the ROC curves\r\nfigure\r\n% start with the full feature set from the previous computation\r\n[rocX,rocY,~,auc] = perfcurve(Ytest,Yscore(:,posOrder),posLabel);\r\ncurves(7) = plot(rocX,rocY,'Color',colors(end,:));\r\nlabels{7} = sprintf('Features:%d, AUC: %.4f',width(Xtrain),auc); \r\nhold on\r\n% test with various number of features\r\nfeatSize = [3,5,10,15,20,25];\r\nfor i = 1:length(featSize)\r\n    % use 50 trees and fewer options to make it go faster\r\n    model = TreeBagger(50,table2array(Xtrain(:,sort(idxvarimp(1:featSize(i))))),...\r\n        Ytrain,'Method','classification','options',opts);\r\n    % non parallel version\r\n%     model = TreeBagger(50,table2array(Xtrain(:,sort(idxvarimp(1:featSize(i))))),...\r\n%         Ytrain,'Method','classification');\r\n    % get the classification scores\r\n    [~,Yscore] = predict(model,table2array(Xtest(:,sort(idxvarimp(1:featSize(i))))));\r\n    % compute and plot the ROC curve and AUC score\r\n    [rocX,rocY,~,auc] = perfcurve(Ytest,Yscore(:,posOrder ),posLabel);\r\n    curves(i) = plot(rocX,rocY,'Color',colors(i,:));\r\n    % get the labels for legend\r\n    labels{i} = sprintf('Features:%02d, AUC: %.4f',featSize(i),auc); \r\nend\r\nhold off\r\nxlabel('False posiitve rate');\r\nylabel('True positive rate')\r\ntitle(sprintf('ROC curve for Class ''%s'', predicted vs. actual',posLabel))\r\nlegend(curves,labels,'Location','East');\r\n```\r\n![ROC Plot](html/HAR_05.png)\r\n\r\nThe reduced model with 12 features\r\n----------------------------------\r\n\r\nBased on the previous analysis, it looks like you can achieve high accuracy rate even if you use as few as 10 features. Let's say we settled for 12 features. We now know you don't have to use the data from the glove for prediction, so that's one less sensor our hypothetical end users would have to buy. Given this result, I may even consider dropping the arm band, and just stick with the belt and dumbbell sensors.\r\n\r\n```\r\n% model with 12 features\r\ntop12 = TreeBagger(50,table2array(Xtrain(:,sort(idxvarimp(1:12)))),...\r\n        Ytrain,'Method','classification','options',opts);\r\n% non-parallel version    \r\n% top12 = TreeBagger(50,table2array(Xtrain(:,sort(idxvarimp(1:12)))),...\r\n%         Ytrain,'Method','classification');\r\n\r\n% compute the confusion matrix and prediction accuracy\r\nYpred = predict(top12,table2array(Xtest(:,sort(idxvarimp(1:12)))));\r\nC = confusionmat(Ytest,categorical(Ypred));\r\ndisp(array2table(C,'VariableNames',rfmodel.ClassNames,'RowNames',rfmodel.ClassNames))\r\nfprintf('Prediction accuracy on the test set: %f\\n\\n', sum(C(logical(eye(5))))/sum(sum(C)))\r\n\r\n% show which features were included\r\ndisp(table(varimp(idxvarimp(1:12)),'RowNames',vars(idxvarimp(1:12)),...\r\n    'VariableNames',{'Importance'}));\r\n\r\n% shut down the parallel pool \r\ndelete(poolobj);\r\n```\r\n```\r\n          A       B      C      D      E \r\n         ____    ___    ___    ___    ___\r\n\r\n    A    1133      0      0      0      1\r\n    B       5    720      7      1      0\r\n    C       0      8    640      3      0\r\n    D       2      0      6    651      0\r\n    E       0      0      0      1    746\r\n\r\nPrediction accuracy on the test set: 0.991335\r\n\r\n                         Importance\r\n                         __________\r\n\r\n    roll_belt            2.7887    \r\n    yaw_belt             2.6094    \r\n    pitch_belt           2.3812    \r\n    gyros_arm_y          2.2761    \r\n    magnet_dumbbell_z    2.2583    \r\n    magnet_dumbbell_y    1.9959    \r\n    pitch_forearm         1.941    \r\n    gyros_forearm_y      1.7463    \r\n    magnet_arm_z         1.7445    \r\n    gyros_dumbbell_x     1.6996    \r\n    accel_dumbbell_y     1.6906    \r\n    gyros_dumbbell_z     1.6339    \r\n```\r\n\r\nConclusion and the next steps - integrate your code into your app\r\n----------------------------------------------------------------\r\n\r\nDespite my initial misgivings about the data, we were able to high prediction accuracy with Random Forest model with just 12 features. However, Random Forest is probably not an ideal model to implement on a mobile app given its memory foot print and slow response time.\r\n\r\nThe next step is to find a simpler models, such as [logistics regression](http://www.mathworks.com/help/stats/mnrfit.html), that can perform decently. You may need to do more preprocessing of the data to make it work.\r\n\r\nFinally, I have never tried this before, but you could generate C code out of MATLAB to incorporate it into an iPhone app. Watch this webinar for more details. I believe there will be an Android version of this webinar eventually. [MATLAB to iPhone Made Easy](http://www.mathworks.com/videos/matlab-to-iphone-made-easy-90834.html)\r\n\r\n![MATLAB to iPhone Made Easy Webinar](html/iphoneWebinar.png)\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}